# **Module 3 â€“ Dimensionality Reduction and Artificial Neural Networks**

## **1. Principal Component Analysis (PCA)**

Principal Component Analysis is a dimensionality reduction technique that transforms high-dimensional data into a smaller set of uncorrelated components. It captures maximum variance with fewer dimensions, improving efficiency while preserving important information.

---

## **2. Linear Discriminant Analysis (LDA)**

Linear Discriminant Analysis reduces dimensionality by projecting data onto axes that maximize separation between classes. It is particularly useful for classification tasks where distinguishing between categories is essential.

---

## **3. Canonical Correlation Analysis (CCA)**

Canonical Correlation Analysis identifies relationships between two sets of variables by finding linear combinations that maximize their correlation. It helps reveal hidden dependencies across datasets in multivariate analysis.

---

## **4. Introduction to Artificial Neural Network**

Artificial Neural Networks are computational models inspired by the functioning of the human brain, consisting of interconnected processing nodes called neurons. They learn complex patterns from data through weighted connections and iterative training.

---

## **5. Understanding the Brain**

Biological brains process information through networks of neurons that communicate using electrical and chemical signals. This structure inspires artificial neural architectures designed to mimic learning and pattern recognition.

---

## **6. Perceptron**

A perceptron is the simplest neural model that performs binary classification by computing a weighted sum of inputs followed by an activation function. It forms the foundational building block for more complex neural networks.

---

## **7. Multi-Layer Perceptron as Universal Approximator**

A Multi-Layer Perceptron (MLP) consists of input, hidden, and output layers capable of learning non-linear relationships. With sufficient neurons, it can approximate any continuous function, making it a universal approximator.

---

## **8. General Architecture of Artificial Neural Network**

An ANN architecture includes layers of interconnected neurons, weight parameters, activation functions, and forward/backward propagation mechanisms. These components work together to extract patterns and make predictions.

---

## **9. Feedforward and Backpropagation**

Feedforward propagation passes inputs through network layers to generate outputs, while backpropagation adjusts weights based on error gradients. Together, they enable neural networks to learn from data efficiently.

---

## **10. Activation Functions (Linear and Nonlinear)**

Activation functions determine how input signals are transformed within neurons to introduce nonlinearity. Common functions include sigmoid, ReLU, tanh, and softmax, each helping networks learn complex mappings in binary and multi-class tasks.

---
