# **Module 2 – Supervised and Unsupervised Learning**

## **1. Introduction to Supervised Learning**

Supervised learning is a learning approach where models are trained using labeled datasets that define clear input–output relationships. It helps the system learn patterns by minimizing prediction errors through repeated adjustments.

---

## **2. Learning a Class From Examples**

Learning a class from examples involves training the model using labeled samples that represent different categories. The algorithm generalizes these examples to classify new data accurately.

---

## **3. Learning Multiple Classes**

Multi-class learning enables the model to differentiate among several output categories instead of just two. It uses strategies like softmax prediction or one-vs-rest to assign inputs to the correct class.

---

## **4. Model Selection and Generalization**

Model selection refers to choosing the most suitable model based on performance on validation data. Generalization measures how well the chosen model performs on new and unseen data without memorizing the training set.

---

## **5. Linear Regression & Feature Selection**

### **Linear Regression**

Linear regression establishes a linear relationship between input variables and a continuous output. It finds parameters that best fit the data by minimizing prediction errors across the dataset.

### **Feature Selection**

Feature selection is the process of choosing the most influential attributes that contribute to accurate predictions. It improves accuracy, reduces overfitting, and simplifies computations.

---

## **6. Bayesian Learning**

Bayesian learning applies Bayes’ theorem to update the probability of hypotheses based on new evidence. It provides a systematic way to handle uncertainty and refine predictions.

---

## **7. Decision Tree Learning**

Decision tree learning builds a hierarchical structure where each internal node represents a feature test and each leaf represents a predicted output. It uses splitting measures like information gain and Gini index.

---

## **8. Classification Tree**

A classification tree predicts categorical labels by splitting data into branches that maximize class purity. It offers simple and interpretable decision paths for classification tasks.

---

## **9. Regression Tree**

A regression tree predicts numeric values by dividing data into regions and assigning each region an average output. It handles nonlinear patterns effectively in continuous-valued predictions.

---

## **10. Multivariate Methods for Learning**

Multivariate learning methods consider multiple input features together to learn complex relationships. Techniques include multivariate classification, multivariate regression, logistic regression, and discriminant analysis.

---

## **11. Introduction to Unsupervised Learning**

Unsupervised learning extracts hidden patterns from unlabeled data without predefined output categories. It is useful in structure discovery, grouping, and dimensionality reduction.

---

## **12. Clustering**

Clustering groups similar data points together based on distance or similarity measures. It is used for customer segmentation, anomaly detection, and document grouping.

---

## **13. Mixture Densities**

Mixture density models represent data distributions as combinations of simpler probability distributions. They allow flexible modeling of complex real-world datasets.

---

## **14. K-Means Clustering**

K-means clustering partitions data into *k* groups by assigning each point to the nearest centroid and updating centroid positions iteratively. It works best for well-separated spherical clusters.

---

## **15. Expectation-Maximization (EM) Algorithm**

The EM algorithm estimates missing or hidden variables to refine model parameters in probabilistic models. It alternates between expectation and maximization steps until convergence.

---

## **16. Latent Dirichlet Allocation (LDA)**

LDA is a probabilistic model used for identifying topics hidden within a collection of documents. It assumes each document contains a mixture of topics represented through word distributions.

---

## **17. Spectral Clustering**

Spectral clustering uses eigenvalues of similarity matrices to identify clusters that may not be linearly separable. It is effective for detecting arbitrarily shaped clusters.

---

## **18. Hierarchical Clustering**

Hierarchical clustering arranges data into a hierarchy of clusters using bottom-up (agglomerative) or top-down (divisive) approaches. The output is a dendrogram showing multi-level grouping.

---
